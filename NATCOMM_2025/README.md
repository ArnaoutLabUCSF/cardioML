# Self-supervised learning for cardiac ultrasound segmentation without manual labels 

Segmentation and measurement of cardiac chambers is critical in echocardiography but is also laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations, while self-supervised approaches have fared poorly in ultrasound to date. To overcome these challenges, we built a pipeline for self-supervised (no manual labels required) segmentation of cardiac chambers, combining computer vision techniques, clinical domain knowledge, and deep learning employing successive networks and early stopping.

## License
CC BY-NC-SA 4.0
This license requires that reusers give credit to the creator. It allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, for noncommercial purposes only. If others modify or adapt the material, they must license the modified material under identical terms.

## Cite
Ferreira, D.L., Lau, C., Salaymang, Z. et al. Self-supervised learning for label-free segmentation in cardiac ultrasound. Nat Commun 16, 4070 (2025). https://doi.org/10.1038/s41467-025-59451-5

## System Requirements
Python version: 3.7.16

To install requirements:
```bash
pip install -r requirements.txt
```

If error during:
```python
import cv2
```
```python
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/ec2-user/miniconda3/envs/ssl-seg/lib/python3.7/site-packages/cv2/__init__.py", line 3, in <module>
    from .cv2 import *
ImportError: libSM.so.6: cannot open shared object file: No such file or directory
```

run
```bash
pip uninstall opencv-python-headless
pip install opencv-python-headless==4.2.0.32
```


## Dataset

We have provided a very small set of example data that is used to run and demonstrate how the code runs. However, some steps of this pipeline are dependent on usable outputs from previous steps. For these steps, we use labels pre-generated by "real" models.


## Model Generation Steps
Each notebook has a prefix with the numbers used below.

1. **Generate watershed labels**
   ```bash
   python 01_generate_watershed_labels.py --indir example_data/A2C/a2c_images
   ```
   - Input: Directory of A2C images (.npy)
      - Example: example_data/a2c_images
   - Output: Directory of watershed masks (.npy)
      - Example: example_data/a2c_watershed_labels

2. **Quality control of watershed labels**
   ```bash
   python 02_quality_control_watershed_labels.py --indir example_data/A2C/a2c_watershed_labels
   ```
   - Input: 
      - Directory of A2C watershed labels e.g., 'example_data/a2c_watershed_labels'
   - Output:
      - CSV containing filenames which passed QC e.g., 'example_data/a2c_watershed_labels_passed_qc.csv'

3. **Visualize watershed labels**
   - File to run: **03_visualize_watershed_labels.ipynb**
   - Inputs:
      - Directory of A2C watershed labels
      - CSV containing filenames which passed QC
   - Outputs:
      - Step-wise visualized watershed labels (in notebook)

4. **Train A2C Unet model**
   - File to run: **04_TrainingUnetA2C.ipynb**
   - Inputs:
      - A2C images
      - Watershed masks
      - Self-learning labels e.g., 'example_data/a2c_step2_labels'
   - Output:
      - 1st round A2C Unet model from watershed labels e.g., 'example_models/1stRound_UNet_A2CModel_Example.h5'
      - 2nd round A2C Unet model from self-learning labels e.g., 'example_models/2ndRound_UNet_A2CModel_Self_learning_Example.h5'
      - Masks from final segmentation model on example training data e.g., 'example_data/a2c_final_inference'
   - <span style="color:red">*Note: The cell that generates self-learning labels is commented out, because the model trained from our small example dataset cannot provide viable labels. Uncomment it after supplying more data.*</span>

5. **Visualize A2C Training Labels**
   - File to run: **05_visualize_a2c_labels.ipynb**
   - Inputs:
      - A2C images
      - Watershed masks
      - Inferred masks on training data from final model
   - Output:
      - Visual comparison of masks in notebook
   
6. **Generate A4C labels from A2C model**

   - <span style="color:red">*Note: The weak labels used in the example were pre-generated using an actual model. The example A2C model was not trained on enough data to provide viable labels. **When following example pipeline, skip this step.***</span>
   ```bash
   python 06_generate_a4c_labels_from_a2c_model.py --weights example_models/A2C/2ndRound_UNet_A2CModel_Self_learning_Example.h5 --indir example_data/A4C/a4c_images
   ```
   - Input:
      - A4C images e.g., 'example_data/A4C/a4c_images'
      - Trained A2C model e.g., 'example_models/A2C/2ndRound_UNet_A2CModel_Self_learning_Example.h5'
   - Output:
      - Weak A4C labels from A2C model e.g., 'example_data/A4C/a4c_step1_labels'

7. **Quality control of weak A4C labels**
   ```bash
   python 07_quality_control_a4c_from_a2c.py --indir example_data/A4C/a4c_step1_labels
   ```
   - Input:
      - A4C step1 labels
   - Output:
      - CSV containing A4C step1 labels that passed QC e.g., 'example_data/A4C/a4c_step1_labels_passed_qc.csv'
      - RV stretched A4C labels e.g., 'example_data/A4C/a4c_step1_labels_stretched_rv'

8. **Train A4C Unet model**
   - File to run: **08_TrainingUnetA4C.ipynb**
   - Input: 
      - A4C images
      - RV stretched A4C labels
      - A4C QC CSV
      - Self learning A4C labels e.g., 'example_data/A4C/a4c_step2_labels'
   - Output:
      - 1st round A4C model from labels generated by A2C model
      - 2nd round A4C model from self-learning labels
      - Inferred masks on training data from final model e.g., 'example_data/A4C/a4c_final_inference'
   - <span style="color:red">*Note: The cell that generates self-learning labels is commented out, because the model trained from our small example dataset cannot provide viable labels. Uncomment it after supplying more data.*</span>

9. **Visualize A4C labels**
   - File to run: **09_visualize_a4c_labels.ipynb**
   - Input:
      - A4C images
      - A4C labels (pre-stretch)
      - A4C labels (post-stretch)
      - A4C final labels from trained model
   - Output:
      - Visual comparison in notebook

10. **Generate hough circle labels**
   ```bash
   python 10_generate_hough_circle_labels.py --indir example_data/SAX/sax_images
   ```
   - Input:
      - SAXMID images e.g., 'example_data/SAX/sax_images'
   - Output:
      - Hough circle labels e.g., 'example_data/SAX/sax_hough_circle_labels'

11. Visualize Sax Hough Circle weak labels.
   - File to run: **11_visualize_Step2_HoughCircles.ipynb**
   - Input:
      - SAXMID images
   - Output:
      - Hough circle visualization in notebook

12. Train HED model for SAXMID images.
   - File to run: **10_training_hed_Saxmid.ipynb**
   - Input:
      - SAXMID images
      - Hough circle labels
   - Output:
      - Trained HED model e.g., 'example_models/SAX/step2_hed_label_hough_circle.h5'

13. Generate filled HED labels
   - <span style="color:red">*Note: The filled hed labels used in the example were pre-generated using an actual model. The example HED model was not trained on enough data to provide viable labels. **When following example pipeline, skip this step.***</span>

   ```bash
   python 13_generate_filled_hed.py --indir example_data/SAX/sax_images --weights example_models/SAX/step2_hed_label_hough_circle.h5
   ```
   - Input:
      - SAXMID images
      - Trained HED model
   - Output:
      - Filled hed labels e.g., 'example_data/SAX/sax_filled_hed_labels'

14. Visualize Filled Hed Predictions
   - File to run: **14_visualize_Step3_FilledHedPrediction.ipynb**
   - Input:
      - SAXMID images
      - Filled hed labels, shapes, masks generte in step 13

15. Train UNET model for SAXMID images
   - File to run: **15_Training_unet_Saxmid.ipynb**
   - Input:
      - SAXMID images
      - Filled hed labels e.g., example_data/SAX/sax_filled_hed_labels
      - Self-learning labels e.g., example_data/SAX/sax_filled_donut_unet
   - Output:
      - Trained SAXMID Unet model e.g., example_models/SAX/step3_unet_filled_donut_pred_unet.h5

16. Generate Measurements on Test Data

   - <span style="color:red">*Note: The measurement csv files used in the example were pre-generated using an actual model. The example models were not trained on enough data to provide viable labels. **When following example pipeline, skip this step.***</span>

   ```bash
   python 16_RunInference_ComputeMeasurements.py --indir test_data --path_to_models real_models/
   ```

   - Inputs:
      - A4C, A2C, SAXMID images
      - Trained A4C, A2C, SAXMID models

   - Outputs:
      - Measurements per view, csv, subdir
      - Measurements with R2 measure on sinusoidal model fit to area by frame

17. Plot Measurement Code

   - Files to run:
      - **17_LV_Volumetrics_Plot_Results.ipynb**
      - **18_LA_Volumetrics_Plot_Results.ipynb**
      - **19_RA_Volumetrics_Plot_Results.ipynb**
      - **20_RV_Area_Plot_Results.ipynb**
      - **21_LV_Mass_Plot_Results.ipynb**

   - Inputs:
      - Measurement csvs generated in previous step
      - Echo measurements from clinical studies
   
   - Outputs:
      - Bland Altman plots comparing clinical and modeled measurements
      - Confusion Matrix showing model accuracy
